{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStreamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir='./ufc10', split='train', transform=None, flow_transform=None, stack_flows=True):\n",
    "        self.video_paths = sorted(glob(f'{root_dir}/videos/{split}/*/*.avi'))\n",
    "        self.df = pd.read_csv(f'{root_dir}/metadata/{split}.csv')\n",
    "        self.split = split\n",
    "        self.n_sampled_frames = 10\n",
    "        self.transform = transform\n",
    "        self.stack_flows = stack_flows\n",
    "        self.flow_transform = flow_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def _get_meta(self, attr, value):\n",
    "        return self.df.loc[self.df[attr] == value]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        video_name = video_path.split('/')[-1].split('.avi')[0]\n",
    "        video_meta = self._get_meta('video_name', video_name)\n",
    "        label = video_meta['label'].item()\n",
    "\n",
    "        frames = self.get_frames(idx)\n",
    "        flows_npy = self.get_flows(idx)\n",
    "\n",
    "        return {\n",
    "            'frames': frames,\n",
    "            'flows_npy': flows_npy,\n",
    "            'label': label,\n",
    "            'video_name': video_name\n",
    "        }\n",
    "\n",
    "    def get_flows(self, idx):\n",
    "        flow_dir = self.video_paths[idx].replace('videos', 'flows').replace('.avi', '')\n",
    "        flows = self.load_flows(flow_dir)                  # list of (2,H,W) tensors length T-1\n",
    "        if self.flow_transform:\n",
    "            flows = [self.flow_transform(f) for f in flows]\n",
    "\n",
    "        if self.stack_flows:\n",
    "            f = torch.stack(flows).contiguous()         # (T-1, 2, H, W)\n",
    "            f = f.view(-1, f.shape[2], f.shape[3])      # (2*(T-1), H, W) = u1,v1,u2,v2,...\n",
    "            return f\n",
    "        return flows\n",
    "\n",
    "    def load_flows(self, frames_dir):\n",
    "        flows = []\n",
    "        for i in range(1, self.n_sampled_frames):\n",
    "            fpath = os.path.join(frames_dir, f\"flow_{i}_{i+1}.npy\")\n",
    "            arr = np.load(fpath).astype(np.float32)\n",
    "            flows.append(torch.from_numpy(arr).contiguous())\n",
    "        return flows\n",
    "\n",
    "    def get_frames(self, idx):\n",
    "        frame_dir = self.video_paths[idx].replace('videos','frames').replace('.avi','')\n",
    "        mid = (self.n_sampled_frames + 1)//2\n",
    "        img = Image.open(os.path.join(frame_dir, f\"frame_{mid}.jpg\")).convert(\"RGB\")\n",
    "        return self.transform(img) if self.transform else T.ToTensor()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hflip_flow(flow, p=0.5):\n",
    "    # flow: (2,H,W)\n",
    "    if torch.rand(1).item() < p:\n",
    "        flow = torch.flip(flow, dims=[2])  # flip width\n",
    "        flow[0] = -flow[0]                 # negate u\n",
    "    return flow\n",
    "\n",
    "def flow_tf_train(flow_2hw: torch.Tensor, out_size=(224,224), clip=20.0):\n",
    "    H0, W0 = flow_2hw.shape[1:]\n",
    "    if (H0, W0) != out_size:\n",
    "        flow = F.interpolate(flow_2hw.unsqueeze(0), size=out_size,\n",
    "                             mode='bilinear', align_corners=False).squeeze(0)\n",
    "        sx, sy = out_size[1]/W0, out_size[0]/H0\n",
    "        flow[0] *= sx; flow[1] *= sy\n",
    "    else:\n",
    "        flow = flow_2hw\n",
    "    # train aug\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        flow = torch.flip(flow, dims=[2]); flow[0] = -flow[0]\n",
    "    flow = torch.clamp(flow, -clip, clip) / clip\n",
    "    return flow\n",
    "\n",
    "def flow_tf_eval(flow_2hw: torch.Tensor, out_size=(224,224), clip=20.0):\n",
    "    H0, W0 = flow_2hw.shape[1:]\n",
    "    if (H0, W0) != out_size:\n",
    "        flow = F.interpolate(flow_2hw.unsqueeze(0), size=out_size,\n",
    "                             mode='bilinear', align_corners=False).squeeze(0)\n",
    "        sx, sy = out_size[1]/W0, out_size[0]/H0\n",
    "        flow[0] *= sx; flow[1] *= sy\n",
    "    else:\n",
    "        flow = flow_2hw\n",
    "    return torch.clamp(flow, -clip, clip) / clip\n",
    "\n",
    "def collate_two_stream(batch):\n",
    "    frames = torch.stack([b['frames'] for b in batch])          # (B, 3, 224, 224)\n",
    "    flows  = torch.stack([b['flows_npy'] for b in batch])       # (B, 2*(T-1), 224, 224)\n",
    "    labels = torch.tensor([b['label'] for b in batch])\n",
    "    names  = [b['video_name'] for b in batch]\n",
    "    return {'frame': frames, 'flows': flows, 'label': labels, 'video_name': names}\n",
    "\n",
    "\n",
    "def make_vgg16(num_classes, in_ch=3, weights=None):\n",
    "    m = models.vgg16(weights=weights)\n",
    "    # Replace first conv if needed\n",
    "    if in_ch != 3:\n",
    "        old = m.features[0]\n",
    "        new = nn.Conv2d(in_ch, old.out_channels, kernel_size=old.kernel_size,\n",
    "                        stride=old.stride, padding=old.padding, bias=False)\n",
    "        with torch.no_grad():\n",
    "            w = old.weight\n",
    "            w_mean = w.mean(1, keepdim=True)\n",
    "            new.weight.copy_(w_mean.repeat(1, in_ch, 1, 1) * (3.0 / in_ch))\n",
    "        m.features[0] = new\n",
    "    m.classifier[6] = nn.Linear(4096, num_classes)\n",
    "    return m\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for b in loader:\n",
    "        x_rgb = b['frame'].to(device, non_blocking=True)\n",
    "        x_flow = b['flows'].to(device, non_blocking=True)\n",
    "        y = b['label'].to(device, non_blocking=True).long()\n",
    "        logits = model(x_rgb, x_flow)\n",
    "        loss_sum += ce(logits, y).item() * y.size(0)\n",
    "        correct  += (logits.argmax(1) == y).sum().item()\n",
    "        total    += y.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "def train_one_epoch(model, loader, opt, device, criterion):\n",
    "    model.train()\n",
    "    for b in loader:\n",
    "        x_rgb = b['frame'].to(device, non_blocking=True)\n",
    "        x_flow = b['flows'].to(device, non_blocking=True)\n",
    "        y = b['label'].to(device, non_blocking=True).long()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(x_rgb, x_flow)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "\n",
    "class TwoStreamFusion(nn.Module):\n",
    "    def __init__(self, spatial, temporal, num_classes, fusion='weighted'):\n",
    "        super().__init__()\n",
    "        self.spatial = spatial\n",
    "        self.temporal = temporal\n",
    "        self.fusion = fusion\n",
    "        if fusion == 'mlp':\n",
    "            self.fuse_head = nn.Sequential(\n",
    "                nn.Linear(2 * num_classes, num_classes)\n",
    "            )\n",
    "        elif fusion == 'weighted':\n",
    "            # learnable positive weights; start equal\n",
    "            self.w_rgb  = nn.Parameter(torch.tensor(0.0))  # softplus -> ~1.0\n",
    "            self.w_flow = nn.Parameter(torch.tensor(0.0))\n",
    "        else:\n",
    "            self.fuse_head = None\n",
    "\n",
    "    def forward(self, frame, flows):\n",
    "        logit_rgb  = self.spatial(frame)     # (B, C)\n",
    "        logit_flow = self.temporal(flows)    # (B, C)\n",
    "\n",
    "        if self.fusion == 'avg':\n",
    "            return (logit_rgb + logit_flow) / 2\n",
    "        elif self.fusion == 'logitsum':\n",
    "            return logit_rgb + logit_flow\n",
    "        elif self.fusion == 'mlp':\n",
    "            x = torch.cat([logit_rgb, logit_flow], dim=1)\n",
    "            return self.fuse_head(x)\n",
    "        elif self.fusion == 'weighted':\n",
    "            # positive weights, normalized to sum=1\n",
    "            w1 = torch.nn.functional.softplus(self.w_rgb)\n",
    "            w2 = torch.nn.functional.softplus(self.w_flow)\n",
    "            s  = (w1 + w2).clamp_min(1e-6)\n",
    "            return (w1/s) * logit_rgb + (w2/s) * logit_flow\n",
    "        else:\n",
    "            raise ValueError(self.fusion)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    frame_tf = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    root_dir = '/dtu/datasets1/02516/ucf101_noleakage'\n",
    "\n",
    "    train_dataset= TwoStreamDataset(root_dir, split='train',transform=frame_tf, flow_transform=flow_tf_train, stack_flows=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_two_stream)\n",
    "    val_dataset = TwoStreamDataset(root_dir, split='val',\n",
    "                                transform=frame_tf, flow_transform=flow_tf_eval, stack_flows=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_two_stream)\n",
    "\n",
    "    batch = next(iter(train_loader))\n",
    "    print('frame:', batch['frame'].shape)  # (B, 3, 224, 224)\n",
    "    print('flows:', batch['flows'].shape)  # (B, 18, 224, 224) if T=10\n",
    "    assert batch['frame'].shape[1] == 3, \"RGB must have 3 channels\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    Frames = 10\n",
    "    num_classes = 10\n",
    "    spatial_model = make_vgg16(num_classes, in_ch=3, weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    temporal_model = make_vgg16(num_classes, in_ch=2*(Frames-1), weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = TwoStreamFusion(spatial_model, temporal_model, num_classes=num_classes, fusion='weighted').to(device)\n",
    "\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    EPOCHS = 10\n",
    "    best = {'acc': 0.0, 'state': None}\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # ---- train\n",
    "        train_one_epoch(model, train_loader, opt, device, criterion)\n",
    "\n",
    "        # ---- validate\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"[Fusion][epoch {epoch:02d}] val_loss={val_loss:.4f}  val_acc={val_acc:.3f}\")\n",
    "\n",
    "        # ---- save best\n",
    "        if val_acc > best['acc']:\n",
    "            best['acc'] = val_acc\n",
    "            best['state'] = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        x_rgb = batch['frame'].to(device)\n",
    "        x_flow = batch['flows'].to(device)\n",
    "        y = batch['label'].to(device).long()\n",
    "        opt.zero_grad()\n",
    "        logits = model(x_rgb, x_flow)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_1 (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
