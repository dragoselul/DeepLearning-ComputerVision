{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB-S4m1ZA8oC"
   },
   "source": [
    "# Exercise 1.3\n",
    "\n",
    "## Classification of CIFAR10 images\n",
    "### Optimizers\n",
    "In this exercise we will classify the images from the CIFAR10 dataset. We will use different optimizers and compare their convergence speed. First we import the libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7z8ctltTA8oS"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dg7mIJnA8os"
   },
   "source": [
    "We always check that we are running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fLXvswP1A8os"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on GPU.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=1337):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9FYktcoA8pB"
   },
   "source": [
    "In this exercise we will classify images from the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. \n",
    "CIFAR10 has 60000 colour images of size 32x32 equally distributed in 10 classes.\n",
    "* You should load this dataset (hint: it is a built-in dataset in pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HR6Y_F1MA8pF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([0.4914, 0.4822, 0.4465])\n",
      "std: tensor([0.2470, 0.2435, 0.2616])\n",
      "Batch: torch.Size([96, 3, 32, 32]) torch.Size([96])\n",
      "dtype: torch.float32 min/max: -1.9892172813415527 2.126786708831787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainset_for_mean_std = datasets.CIFAR10(root=\"./data\", train=True, download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_for_mean_std = torch.utils.data.DataLoader(trainset_for_mean_std, batch_size=50000, shuffle=False)\n",
    "\n",
    "data = next(iter(train_loader_for_mean_std))[0]  # shape [50000, 3, 32, 32]\n",
    "mean = data.mean(dim=[0,2,3])\n",
    "std  = data.std(dim=[0,2,3])\n",
    "\n",
    "print(\"mean:\", mean)\n",
    "print(\"std:\", std)\n",
    "\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "batch_size = 96\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_tf)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=test_tf)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Sanity batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch:\", images.shape, labels.shape)   # expect [128, 3, 32, 32], [128]\n",
    "print(\"dtype:\", images.dtype, \"min/max:\", images.min().item(), images.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7JBaV9XA8pU"
   },
   "source": [
    "* Make a CNN to train on the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HAUxC2nCA8pZ"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels: int, growth_rate: int, drop_rate: float = 0.0):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        inter_channels = 4 * growth_rate  # bottleneck width\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, inter_channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm2d(inter_channels)\n",
    "        self.conv2 = nn.Conv2d(inter_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(F.relu(self.norm1(x), inplace=True))\n",
    "        out = self.conv2(F.relu(self.norm2(out), inplace=True))\n",
    "        if self.drop_rate > 0.0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        # return only the new features (k channels)\n",
    "        return out\n",
    "    \n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers: int, in_channels: int, growth_rate: int, drop_rate: float = 0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        channels = in_channels\n",
    "        for i in range(num_layers):\n",
    "            layers.append(DenseLayer(channels, growth_rate, drop_rate))\n",
    "            channels += growth_rate\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.out_channels = channels  # convenience\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            new_feats = layer(torch.cat(features, dim=1))\n",
    "            features.append(new_feats)\n",
    "        return torch.cat(features, dim=1)  # in + L*k\n",
    "    \n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels: int, compression: float = 0.5):\n",
    "        super().__init__()\n",
    "        out_channels = int(in_channels * compression)\n",
    "        self.norm = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv(self.relu(self.norm(x)))\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNetCIFAR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        growth_rate: int = 24,\n",
    "        block_layers=(16, 16, 16),\n",
    "        compression: float = 0.5,\n",
    "        stem_channels: int = 16,\n",
    "        num_classes: int = 10,\n",
    "        drop_rate: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        # Stem\n",
    "        self.stem = nn.Conv2d(3, stem_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        # Block 1\n",
    "        self.block1 = DenseBlock(block_layers[0], stem_channels, growth_rate, drop_rate)\n",
    "        c1 = self.block1.out_channels\n",
    "        self.trans1 = Transition(c1, compression)\n",
    "        c1p = self.trans1.out_channels  # after compression\n",
    "\n",
    "        # Block 2\n",
    "        self.block2 = DenseBlock(block_layers[1], c1p, growth_rate, drop_rate)\n",
    "        c2 = self.block2.out_channels\n",
    "        self.trans2 = Transition(c2, compression)\n",
    "        c2p = self.trans2.out_channels\n",
    "\n",
    "        # Block 3\n",
    "        self.block3 = DenseBlock(block_layers[2], c2p, growth_rate, drop_rate)\n",
    "        c3 = self.block3.out_channels\n",
    "\n",
    "        # Head\n",
    "        self.norm = nn.BatchNorm2d(c3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.classifier = nn.Linear(c3, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)                     # B, C0, 32, 32\n",
    "        x = self.block1(x)                   # channels grow to C0 + L1*k\n",
    "        x = self.trans1(x)                   # halve channels, 32→16\n",
    "        x = self.block2(x)\n",
    "        x = self.trans2(x)                   # halve channels, 16→8\n",
    "        x = self.block3(x)\n",
    "        x = self.relu(self.norm(x))\n",
    "        x = F.adaptive_avg_pool2d(x, 1).flatten(1)  # B, C3\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IZNf-XPFA8pu"
   },
   "outputs": [],
   "source": [
    "model = DenseNetCIFAR()\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CqWfe3U5A8qB"
   },
   "outputs": [],
   "source": [
    "#We define the training as a function so we can easily re-use it.\n",
    "def train(model, optimizer, num_epochs=10):\n",
    "    def loss_fun(output, target):\n",
    "        return F.nll_loss(torch.log(output), target)\n",
    "    out_dict = {'train_acc': [],\n",
    "              'test_acc': [],\n",
    "              'train_loss': [],\n",
    "              'test_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(data)\n",
    "            #Compute the loss\n",
    "            loss = loss_fun(output, target)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (target==predicted).sum().cpu().item()\n",
    "        #Comput the test accuracy\n",
    "        test_loss = []\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            test_loss.append(loss_fun(output, target).cpu().item())\n",
    "            predicted = output.argmax(1)\n",
    "            test_correct += (target==predicted).sum().cpu().item()\n",
    "        out_dict['train_acc'].append(train_correct/len(trainset))\n",
    "        out_dict['test_acc'].append(test_correct/len(testset))\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['test_loss'].append(np.mean(test_loss))\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB7NHbYfA8qf"
   },
   "source": [
    " * Train the network and plot make a plot of the loss and accuracy for both training and with the epoch on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c053536qA8qk"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ba5b8645b543a39cb7bc851af4ed6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab8251c7baa43e5b043b10df41c7f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out_dict = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[32m      4\u001b[39m plt.legend((\u001b[33m'\u001b[39m\u001b[33mTest error\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mTrain eror\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, num_epochs)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#Update the weights\u001b[39;00m\n\u001b[32m     26\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m train_loss.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#Compute how many were correctly classified\u001b[39;00m\n\u001b[32m     30\u001b[39m predicted = output.argmax(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "out_dict = train(model, optimizer)\n",
    "print(out_dict)\n",
    "plt.plot()\n",
    "plt.legend(('Test error','Train eror'))\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmxbypURA8q2"
   },
   "source": [
    "* Discuss what you see. Are you overfitting to the training data? Do you not learn anything? What can you change to do better?\n",
    "\n",
    "* Repeat the above steps but using Adam as the optimizer. Use Pytorch's defaults parameters. Do you learn faster?\n",
    "* Which optimizer works best for you?\n",
    "* Plot the test and test errors for both SGD and Adam in one plot\n",
    "* Try adding Batch normalisation after your convolutional layers. Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMMyz4WhA8q6"
   },
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQhB2BH3A8q8"
   },
   "source": [
    "Now you will create and train a ResNet.\n",
    "* Implement the Residual block as a network below using convolutional kernel size $3\\times3$ according to the figure below\n",
    "![Residual block](https://cdn-images-1.medium.com/max/800/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-FoRHmrA8rC"
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        ...\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZDp_0SgA8rS"
   },
   "source": [
    "The following code is a sanity of your residual block network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YRnqMaTA8rW"
   },
   "outputs": [],
   "source": [
    "#Sanity test of your implementation\n",
    "C = 4\n",
    "res_block = ResNetBlock(C)\n",
    "assert(len(res_block.state_dict())==4)\n",
    "for name, weight in res_block.state_dict().items():\n",
    "    weight*=0\n",
    "    desired_shape = {'bias': (C,), 'weight': (C, C, 3, 3)}[name.split('.')[-1]]\n",
    "    assert(desired_shape==weight.shape)\n",
    "x = torch.randn(32, C, 32,32)\n",
    "assert(torch.abs(res_block(x)-F.relu(x)).max()==0)\n",
    "print(\"Passed sanity check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKFu5rWDA8rv"
   },
   "source": [
    "We define a network that uses your `ResNetBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waKiIu5NA8r2"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_in, n_features, num_res_blocks=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        #First conv layers needs to output the desired number of features.\n",
    "        conv_layers = [nn.Conv2d(n_in, n_features, kernel_size=3, stride=1, padding=1),\n",
    "                       nn.ReLU()]\n",
    "        for i in range(num_res_blocks):\n",
    "            conv_layers.append(ResNetBlock(n_features))\n",
    "        self.res_blocks = nn.Sequential(*conv_layers)\n",
    "        self.fc = nn.Sequential(nn.Linear(32*32*n_features, 2048),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(2048, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512,10),\n",
    "                                nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.res_blocks(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF1XvsLGA8sH"
   },
   "source": [
    "Let's train our new ResNet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWychvPvA8sN"
   },
   "outputs": [],
   "source": [
    "model = ResNet(3, 8)\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "out_dict = train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoLIRQCr23Es"
   },
   "source": [
    "\n",
    "\n",
    "Do you get nan loss at some point during training? \n",
    "This can be caused by the numerical instability of using softmax and log as two functions. \n",
    "* Change your network and loss to use a layer that combines the softmax log into one such as `nn.LogSoftmax`. You can also use `nn.CrossEntropyLoss` which also integrates `nn.NLLLoss`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 1.3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
