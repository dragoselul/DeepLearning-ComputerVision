{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4 Hotdog -- no hotdog\n",
    "This is the poster hand-in project for the course. Please see the associated PDF for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRIhx7PugJy3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35PhqXpWUZ7I"
   },
   "source": [
    "We always check that we are running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic_gOv_pUZeB"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAj64PJYgJzC"
   },
   "source": [
    "We provide you with a class that can load the *hotdog/not hotdog* dataset you should use from /dtu/datasets1/02516/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mUlnOuzgJzF"
   },
   "outputs": [],
   "source": [
    "class Hotdog_NotHotdog(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path='hotdog_nothotdog'):\n",
    "        'Initialization'\n",
    "        data_path = os.path.join(os.getcwd(), data_path)\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        image_classes = [os.path.split(d)[1] for d in glob.glob(data_path +'/*') if os.path.isdir(d)]\n",
    "        image_classes.sort()\n",
    "        self.name_to_label = {c: id for id, c in enumerate(image_classes)}\n",
    "        self.image_paths = glob.glob(data_path + '/*/*.jpg')\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        c = os.path.split(os.path.split(image_path)[0])[1]\n",
    "        y = self.name_to_label[c]\n",
    "        X = self.transform(image)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute the datasets mean and std. Since neural nets are sensitive to the input scale of the dataset such as the scale of the RGB channels.\n",
    "\n",
    "This should ensure that each channel (R,G,B) are centered around zero with unit variance and should stabilize our training and make the optimizer more predictable.\n",
    "\n",
    "\n",
    "We need first to design a transformer which will resize all the images to the IMG_SIZE. We did this to ensure that the input dimensions are uniform and we have picked 224 for the IMG_SIZE as it is very common and people on the internet say it is a good balance between image detail and not getting the GPU on fire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "stats_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE, antialias=True),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assign a batch size (96 works quite well on this GPU with 8 GB of VRAM -  reduce this number if you get an error about not having enough memory available)\n",
    "We load the training data and we'll proceed to calculate the mean and std for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 96\n",
    "\n",
    "train_ds_stats = Hotdog_NotHotdog(train=True, transform=stats_transform)\n",
    "stats_loader = DataLoader(train_ds_stats, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "sum_c = torch.zeros(3, dtype=torch.float64)\n",
    "sqsum_c = torch.zeros(3, dtype=torch.float64)\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in stats_loader:                 # x: [B,3,H,W]\n",
    "        x = x.to(torch.float64)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        sum_c   += x.sum(dim=[0,2,3])\n",
    "        sqsum_c += (x * x).sum(dim=[0,2,3])\n",
    "        count   += batch_size * height * width\n",
    "\n",
    "mean = (sum_c / count).to(torch.float32)\n",
    "var  = (sqsum_c / count) - (mean.double() ** 2)\n",
    "std  = torch.sqrt(var.clamp_min(1e-8)).to(torch.float32)\n",
    "\n",
    "print(\"MEAN:\", mean.tolist())\n",
    "print(\"STD :\", std.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JewkmhKlgJzN"
   },
   "source": [
    "Let's add the transforms for the training data and for the test data.\n",
    "We will introduce more randomness in the training data, but we want to keep the test data deterministic, so we will just resize and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcilkL3dgJzP"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224, antialias=True),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "])\n",
    "\n",
    "\n",
    "trainset = Hotdog_NotHotdog(train=True, transform=train_transform)\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "testset = Hotdog_NotHotdog(train=False, transform=test_transform)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho-YRb6HgJzZ"
   },
   "source": [
    "Let's look at some images from our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm4Ara7dgJza"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(21):\n",
    "    plt.subplot(5,7,i+1)\n",
    "    plt.imshow(np.swapaxes(np.swapaxes(images[i].numpy(), 0, 2), 0, 1))\n",
    "    plt.title(['hotdog', 'not hotdog'][labels[i].item()])\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12N0EYYsQPhJ"
   },
   "source": [
    "Now create a model and train it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuxX7GAnQHQ6"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),           # ↓ add pooling early: 224→112\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),           # 112→56\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),           # 56→28\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # keep spatial size here: 28→28\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),           # 28→14\n",
    "        )\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)         # [B,128,1,1]\n",
    "        self.fc  = nn.Linear(128, num_classes)     # logits, no Softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)          # [B,128,H,W]\n",
    "        x = self.block6(x)\n",
    "        x = self.gap(x).flatten(1)  # [B,128]\n",
    "        x = nn.Dropout(0.3)(x)\n",
    "        return self.fc(x)           # logits\n",
    "    \n",
    "# class BasicBlock(nn.Module):\n",
    "#     def __init__(self, cin, cout, stride=1):\n",
    "#         super().__init__()\n",
    "#         self.down = None\n",
    "#         if stride != 1 or cin != cout:\n",
    "#             self.down = nn.Sequential(nn.Conv2d(cin, cout, 1, stride, bias=False),\n",
    "#                                       nn.BatchNorm2d(cout))\n",
    "#         self.f = nn.Sequential(\n",
    "#             nn.Conv2d(cin, cout, 3, stride, 1, bias=False), nn.BatchNorm2d(cout), nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(cout, cout, 3, 1, 1, bias=False), nn.BatchNorm2d(cout)\n",
    "#         )\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y = self.f(x)\n",
    "#         if self.down is not None:\n",
    "#             x = self.down(x)\n",
    "#         return self.relu(x + y)\n",
    "\n",
    "# class HotdogResNet(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super().__init__()\n",
    "#         self.stem = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, 3, 2, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU(inplace=True)\n",
    "#         )  # 224→112\n",
    "#         self.stage1 = nn.Sequential(BasicBlock(64, 64), BasicBlock(64, 64))\n",
    "#         self.stage2 = nn.Sequential(BasicBlock(64, 128, stride=2), BasicBlock(128, 128))   # 112→56\n",
    "#         self.stage3 = nn.Sequential(BasicBlock(128, 256, stride=2), BasicBlock(256, 256))  # 56→28\n",
    "#         self.stage4 = nn.Sequential(BasicBlock(256, 512, stride=2), BasicBlock(512, 512)) # 28→14\n",
    "#         self.head = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "#                                   nn.Dropout(0.3), nn.Linear(512, num_classes))\n",
    "#     def forward(self, x):\n",
    "#         x = self.stem(x); x = self.stage1(x); x = self.stage2(x); x = self.stage3(x); x = self.stage4(x)\n",
    "#         return self.head(x)  # logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the training as a function so we can easily re-use it.\n",
    "def train(model, optimizer, num_epochs=10):\n",
    "    out_dict = {'train_acc': [],\n",
    "              'test_acc': [],\n",
    "              'train_loss': [],\n",
    "              'test_loss': []}\n",
    "    \n",
    "    use_cuda = device.type == \"cuda\"\n",
    "    use_amp  = use_cuda  # flip to False if you want to disable AMP\n",
    "    scaler   = torch.amp.GradScaler(enabled=use_amp)\n",
    "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "\n",
    "    if use_cuda:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            ctx = torch.amp.autocast(device_type=\"cuda\",dtype=torch.float16) if use_amp else torch.autocast(\"cpu\", enabled=False)\n",
    "            with ctx:\n",
    "                logits = model(data)                 # model should return logits, not softmax\n",
    "                loss   = criterion(logits, target)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(data)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (target==predicted).sum().cpu().item()\n",
    "        #Comput the test accuracy\n",
    "        test_loss = []\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            ctx = torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16) if use_amp else torch.autocast(\"cpu\", enabled=False)\n",
    "            with ctx:\n",
    "                for data, target in test_loader:\n",
    "                    data   = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    logits = model(data)\n",
    "                    loss   = criterion(logits, target)\n",
    "                    test_loss.append(loss.item())\n",
    "                    test_correct += (logits.argmax(1) == target).sum().item()\n",
    "\n",
    "        out_dict['train_acc'].append(train_correct / len(train_loader.dataset))\n",
    "        out_dict['test_acc'].append(test_correct / len(test_loader.dataset))\n",
    "        out_dict['train_loss'].append(float(np.mean(train_loss)))\n",
    "        out_dict['test_loss'].append(float(np.mean(test_loss)))\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = train(model, optimizer, num_epochs=100)\n",
    "print(out_dict)\n",
    "plt.plot(out_dict['train_acc'], label='Train accuracy')\n",
    "plt.plot(out_dict['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Accuracy')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project 1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
